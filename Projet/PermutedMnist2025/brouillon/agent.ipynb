{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GVHyBXrknC8Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from permuted_mnist.env.permuted_mnist import PermutedMNISTEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "eziyIgIhnLVh",
    "outputId": "37cefd70-a572-4b6d-8942-958d02291614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created with 10 permuted tasks\n",
      "Training set size: 60000 samples\n",
      "Test set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "env = PermutedMNISTEnv(number_episodes=10)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "env.set_seed(42)\n",
    "\n",
    "print(f\"Environment created with {env.number_episodes} permuted tasks\")\n",
    "print(f\"Training set size: {env.train_size} samples\")\n",
    "print(f\"Test set size: {env.test_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task structure:\n",
      "- X_train shape: (60000, 28, 28)\n",
      "- y_train shape: (60000, 1)\n",
      "- X_test shape: (10000, 28, 28)\n",
      "- y_test shape: (10000,)\n",
      "\n",
      "Label distribution in training set:\n",
      "  Label 0: 6131 samples\n",
      "  Label 1: 6742 samples\n",
      "  Label 2: 5421 samples\n",
      "  Label 3: 5851 samples\n",
      "  Label 4: 6265 samples\n",
      "  Label 5: 5958 samples\n",
      "  Label 6: 5949 samples\n",
      "  Label 7: 5842 samples\n",
      "  Label 8: 5923 samples\n",
      "  Label 9: 5918 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = env.get_next_task()\n",
    "\n",
    "print(\"Task structure:\")\n",
    "print(f\"- X_train shape: {task['X_train'].shape}\")\n",
    "print(f\"- y_train shape: {task['y_train'].shape}\")\n",
    "print(f\"- X_test shape: {task['X_test'].shape}\")\n",
    "print(f\"- y_test shape: {task['y_test'].shape}\")\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "unique, counts = np.unique(task['y_train'], return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Label {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: The images look scrambled due to pixel permutation!\n",
      "The labels are also permuted (not the original MNIST labels).\n"
     ]
    }
   ],
   "source": [
    "# Display some examples from the permuted task\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Permuted MNIST Examples (Task 1)', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(task['X_train'][i], cmap='gray')\n",
    "    ax.set_title(f'Label: {task[\"y_train\"][i][0]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The images look scrambled due to pixel permutation!\")\n",
    "print(\"The labels are also permuted (not the original MNIST labels).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  Agent (Baseline)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     20\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mrandom_agent\u001b[49m.train(task[\u001b[33m'\u001b[39m\u001b[33mX_train\u001b[39m\u001b[33m'\u001b[39m], task[\u001b[33m'\u001b[39m\u001b[33my_train\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     25\u001b[39m predictions = random_agent.predict(task[\u001b[33m'\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     27\u001b[39m elapsed_time = time.time() - start_time\n",
      "\u001b[31mNameError\u001b[39m: name 'random_agent' is not defined"
     ]
    }
   ],
   "source": [
    "import time # <--- BONNE LIGNE\n",
    "env.reset()\n",
    "env.set_seed(42)\n",
    "\n",
    "Agent = Agent(output_dim=10, seed=42)\n",
    "\n",
    "\n",
    "random_accuracies = []\n",
    "random_times = []\n",
    "\n",
    "print(\"Evaluating  Agent (Baseline)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "task_num = 1\n",
    "while True:\n",
    "    task = env.get_next_task()\n",
    "    if task is None:\n",
    "        break\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    random_agent.train(task['X_train'], task['y_train'])\n",
    "    \n",
    "    predictions = random_agent.predict(task['X_test'])\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    accuracy = env.evaluate(predictions, task['y_test'])\n",
    "    \n",
    "    random_accuracies.append(accuracy)\n",
    "    random_times.append(elapsed_time)\n",
    "    \n",
    "    print(f\"Task {task_num}: Accuracy = {accuracy:.2%}, Time = {elapsed_time:.4f}s\")\n",
    "    task_num += 1\n",
    "\n",
    "print(f\"\\nRandom Agent Summary:\")\n",
    "print(f\"  Mean accuracy: {np.mean(random_accuracies):.2%} ± {np.std(random_accuracies):.2%}\")\n",
    "print(f\"  Total time: {np.sum(random_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    \n",
    "    # 1. Statistiques globales\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    \n",
    "    # 2. Comptage de pixels\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    \n",
    "    # Retourne un tableau de (N_samples, 5 features)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent ExtraTrees-Seul + 5 \"Super-Features\" (moyenne, std, etc.).\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        \n",
    "        # --- Paramètres ET (puissants, car on a le temps) ---\n",
    "        self.params = {\n",
    "            'n_estimators': 150,    # Comme notre baseline (27s)\n",
    "            'max_depth': 25,\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset l'agent pour une nouvelle tâche.\"\"\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'ET sur les 784 pixels + 5 features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (Ton idée)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features)) # 789 features\n",
    "        \n",
    "        # 4. Créer et entraîner le modèle\n",
    "        self.model = ExtraTreesClassifier(**self.params)\n",
    "        self.model.fit(X_hybrid, y_flat)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec l'ET hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent ExtraTrees + 5 \"Super-Features\"\n",
    "    On utilise notre budget temps en augmentant les arbres.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        \n",
    "        # --- ON UTILISE LE BUDGET TEMPS ---\n",
    "        self.params = {\n",
    "            'n_estimators': 275,    # Augmenté (avant: 150) pour viser ~60s\n",
    "            'max_depth': 25,\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset l'agent pour une nouvelle tâche.\"\"\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'ET sur les 784 pixels + 5 features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features)) # 789 features\n",
    "        \n",
    "        # 4. Créer et entraîner le modèle\n",
    "        self.model = ExtraTreesClassifier(**self.params)\n",
    "        self.model.fit(X_hybrid, y_flat)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec l'ET hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 9 'super-features' globales.\"\"\"\n",
    "    \n",
    "    # 1. Statistiques de base\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    \n",
    "    # 2. Comptage de pixels\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    \n",
    "    # 3. Percentiles\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "    \n",
    "    # 4. Moments de distribution (Asymétrie et Aplatissement)\n",
    "    skewness = stats.skew(X_flat, axis=1)\n",
    "    kurt = stats.kurtosis(X_flat, axis=1)\n",
    "    \n",
    "    # Retourne un tableau de (N_samples, 9 features)\n",
    "    return np.vstack((\n",
    "        mean, std, median, count_zero, count_max, q1, q3, skewness, kurt\n",
    "    )).T\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent ExtraTrees-Seul + 9 \"Super-Features\".\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        \n",
    "        # On garde les mêmes paramètres que l'agent à 33s\n",
    "        self.params = {\n",
    "            'n_estimators': 150,\n",
    "            'max_depth': 25,\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset l'agent pour une nouvelle tâche.\"\"\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'ET sur les 784 pixels + 9 features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (v2)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features)) # 793 features\n",
    "        \n",
    "        # 4. Créer et entraîner le modèle\n",
    "        self.model = ExtraTreesClassifier(**self.params)\n",
    "        self.model.fit(X_hybrid, y_flat)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec l'ET hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    \n",
    "    # 1. Statistiques globales\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    \n",
    "    # 2. Comptage de pixels\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    \n",
    "    # Retourne un tableau de (N_samples, 5 features)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        \n",
    "        # --- Paramètres équilibrés Vitesse/Précision ---\n",
    "        self.params = {\n",
    "            'n_estimators': 1000,   # Plafond (s'arrêtera avant)\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 64,       # Plus rapide que 128\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'subsample': 0.7,\n",
    "        }\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.validation_size = 0.15\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset l'agent pour une nouvelle tâche.\"\"\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne le LGBM sur les 784 pixels + 5 features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (Ton idée)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features))\n",
    "\n",
    "        # 4. Créer le set de validation pour l'arrêt précoce\n",
    "        X_base, X_val, y_base, y_val = train_test_split(\n",
    "            X_hybrid, y_flat, \n",
    "            test_size=self.validation_size, \n",
    "            random_state=self.seed, \n",
    "            stratify=y_flat\n",
    "        )\n",
    "        \n",
    "        # 5. Créer et entraîner le modèle\n",
    "        self.model = lgb.LGBMClassifier(**self.params)\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_base, \n",
    "            y_base,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='multi_logloss',\n",
    "            callbacks=[lgb.early_stopping(self.early_stopping_rounds, verbose=False)]\n",
    "        )\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec le LGBM hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ===================================================================\n",
    "# AGENT \"LGBM-SIMPLE + FEATURES\"\n",
    "# ===================================================================\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent LGBM-Simple (`num_leaves=31`) + 5 \"Super-Features\".\n",
    "    C'est notre test le plus rapide pour le LGBM.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        \n",
    "        # --- Paramètres SIMPLES pour la VITESSE ---\n",
    "        self.params = {\n",
    "            'n_estimators': 1000,   # Plafond (s'arrêtera avant)\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,       # PLUS RAPIDE (était 64)\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "        }\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.validation_size = 0.15\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne le LGBM sur les 784 pixels + 5 features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (Ton idée)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features))\n",
    "\n",
    "        # 4. Créer le set de validation pour l'arrêt précoce\n",
    "        X_base, X_val, y_base, y_val = train_test_split(\n",
    "            X_hybrid, y_flat, \n",
    "            test_size=self.validation_size, \n",
    "            random_state=self.seed, \n",
    "            stratify=y_flat\n",
    "        )\n",
    "        \n",
    "        # 5. Créer et entraîner le modèle\n",
    "        self.model = lgb.LGBMClassifier(**self.params)\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_base, \n",
    "            y_base,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='multi_logloss',\n",
    "            callbacks=[lgb.early_stopping(self.early_stopping_rounds, verbose=False)]\n",
    "        )\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec le LGBM hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from scipy import stats\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent K-NN (Faiss) entraîné UNIQUEMENT sur les 5 \"Super-Features\".\n",
    "    Hypothèse : Les 784 pixels sont du bruit.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.k = 15 # On a peu de features, on peut prendre plus de voisins\n",
    "        self.index = None\n",
    "        self.y_train_labels = None\n",
    "        faiss.omp_set_num_threads(2) # 2 CPUs\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = None\n",
    "        self.y_train_labels = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne le K-NN sur les 5 features.\n",
    "        \"\"\"\n",
    "        # 1. Aplatir (juste pour le calcul des features)\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (Ton idée)\n",
    "        X_features = create_super_features(X_flat).astype('float32')\n",
    "        \n",
    "        # 3. Normaliser les 5 features (important pour Faiss)\n",
    "        normalize(X_features, norm='l2', axis=1, copy=False)\n",
    "        \n",
    "        self.y_train_labels = y_flat\n",
    "        d = X_features.shape[1] # Dimensions = 5\n",
    "\n",
    "        # 4. Créer l'index Faiss (HNSW rapide)\n",
    "        self.index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_L2)\n",
    "        self.index.add(X_features)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec le K-NN sur 5 features.\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir (pour le calcul des features)\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        X_test_features = create_super_features(X_test_flat).astype('float32')\n",
    "        \n",
    "        # 3. Normaliser les 5 features\n",
    "        normalize(X_test_features, norm='l2', axis=1, copy=False)\n",
    "        \n",
    "        # 4. Prédire (ultra-rapide)\n",
    "        distances, indices = self.index.search(X_test_features, self.k)\n",
    "        neighbor_labels = self.y_train_labels[indices]\n",
    "        predictions, _ = stats.mode(neighbor_labels, axis=1)\n",
    "        \n",
    "        return predictions.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ===================================================================\n",
    "# AGENT \"LGBM + FEATURES + SUB-SAMPLE\" (Corrigé)\n",
    "# ===================================================================\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent LGBM + 5 \"Super-Features\"\n",
    "    Corrigé pour l'erreur \"Multiclass objective\".\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.n_jobs = 2 # 2 CPUs\n",
    "        self.train_fraction = 0.3 # NE S'ENTRAÎNE QUE SUR 30%\n",
    "        \n",
    "        # --- CORRECTION DU BUG ---\n",
    "        self.params = {\n",
    "            'objective': 'multiclass',  # DIRE EXPLICITEMENT\n",
    "            'num_class': 10,            # DIRE EXPLICITEMENT\n",
    "            'metric': 'multi_logloss',  # DIRE EXPLICITEMENT\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 90,\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'random_state': self.seed,\n",
    "        }\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.validation_size_for_early_stopping = 0.15 # 15% du sub-sample\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne le LGBM sur un petit sous-échantillon.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Aplatir\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_flat = y_train.ravel()\n",
    "\n",
    "        # 2. CRÉER LES SUPER-FEATURES (Ton idée)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 3. Combiner les features\n",
    "        X_hybrid = np.hstack((X_flat, super_features))\n",
    "\n",
    "        # 4. --- SOUS-ÉCHANTILLON ---\n",
    "        n_samples = int(X_hybrid.shape[0] * self.train_fraction)\n",
    "        rng = np.random.RandomState(self.seed)\n",
    "        indices = rng.choice(X_hybrid.shape[0], n_samples, replace=False)\n",
    "        X_base_sub = X_hybrid[indices]\n",
    "        y_base_sub = y_flat[indices]\n",
    "        # -------------------------------------------------\n",
    "        \n",
    "        # 5. --- CORRECTION DU BUG (Créer un eval_set explicite) ---\n",
    "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "            X_base_sub, \n",
    "            y_base_sub, \n",
    "            test_size=self.validation_size_for_early_stopping, \n",
    "            random_state=self.seed, \n",
    "            stratify=y_base_sub\n",
    "        )\n",
    "        \n",
    "        # 6. Créer et entraîner le modèle\n",
    "        self.model = lgb.LGBMClassifier(**self.params)\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train_final, \n",
    "            y_train_final,\n",
    "            eval_set=[(X_val, y_val)], # PASSER L'EVAL_SET\n",
    "            # eval_metric='multi_logloss', # Plus besoin, c'est dans les params\n",
    "            callbacks=[lgb.early_stopping(\n",
    "                self.early_stopping_rounds, \n",
    "                verbose=False\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédit avec le LGBM hybride.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Aplatir\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 2. CRÉER LES SUPER-FEATURES pour le test\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_test_hybrid = np.hstack((X_test_flat, super_features_test))\n",
    "        \n",
    "        # 4. Prédire\n",
    "        return self.model.predict(X_test_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, output_dim=10, seed=None):\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        self.model = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def _extract_features(self, X):\n",
    "        \"\"\"Extrait 5 features statistiques ultra-rapides\"\"\"\n",
    "        # Flatten si nécessaire\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        # 5 features rapides et discriminantes\n",
    "        features = np.column_stack([\n",
    "            np.mean(X, axis=1),           # Luminosité moyenne\n",
    "            np.std(X, axis=1),            # Contraste\n",
    "            np.max(X, axis=1),            # Pic d'intensité\n",
    "            np.sum(X > 128, axis=1),      # Nb pixels blancs\n",
    "            np.sum(X > 0, axis=1)         # Nb pixels non-nuls\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Stratégie hybride : LightGBM sur sous-échantillon + features\"\"\"\n",
    "        \n",
    "        # Flatten\n",
    "        if X_train.ndim == 3:\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        \n",
    "        y_train = y_train.ravel()\n",
    "        n_samples = len(X_train)\n",
    "        \n",
    "        # SUB-SAMPLING : Clé de la vitesse de Milton\n",
    "        # Entraîner sur 40% des données suffit pour >98% de précision\n",
    "        sample_ratio = 0.40\n",
    "        sample_size = int(n_samples * sample_ratio)\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        sample_idx = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        \n",
    "        X_sample = X_train[sample_idx]\n",
    "        y_sample = y_train[sample_idx]\n",
    "        \n",
    "        # Features statistiques\n",
    "        features = self._extract_features(X_sample)\n",
    "        \n",
    "        # Combine pixels + features\n",
    "        X_combined = np.hstack([X_sample.astype(np.float32) / 255.0, features])\n",
    "        \n",
    "        # LightGBM optimisé pour vitesse ET précision\n",
    "        self.model = lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            num_class=self.output_dim,\n",
    "            n_estimators=300,           # Équilibre vitesse/précision\n",
    "            num_leaves=80,              # Profondeur optimale\n",
    "            learning_rate=0.05,\n",
    "            max_depth=12,\n",
    "            min_child_samples=10,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=self.seed,\n",
    "            n_jobs=2,                   # Utilise les 2 CPUs\n",
    "            verbose=-1,\n",
    "            force_col_wise=True         # Optimisation CPU\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_combined, y_sample)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prédiction rapide\"\"\"\n",
    "        if self.model is None:\n",
    "            return np.zeros(X_test.shape[0], dtype=np.int32)\n",
    "        \n",
    "        # Flatten\n",
    "        if X_test.ndim == 3:\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Features\n",
    "        features = self._extract_features(X_test)\n",
    "        \n",
    "        # Combine\n",
    "        X_combined = np.hstack([X_test.astype(np.float32) / 255.0, features])\n",
    "        \n",
    "        # Prédiction\n",
    "        predictions = self.model.predict(X_combined)\n",
    "        \n",
    "        return predictions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, output_dim=10, seed=None):\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        self.model = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def _extract_advanced_features(self, X):\n",
    "        \"\"\"Features avancées optimisées pour vitesse\"\"\"\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        X_normalized = X / 255.0\n",
    "        \n",
    "        # Features statistiques de base (ultra-rapides)\n",
    "        mean = np.mean(X_normalized, axis=1, keepdims=True)\n",
    "        std = np.std(X_normalized, axis=1, keepdims=True)\n",
    "        \n",
    "        # Features géométriques (discriminantes)\n",
    "        # Densité par quadrants (28x28 -> 4 zones de 14x14)\n",
    "        X_img = X_normalized.reshape(-1, 28, 28)\n",
    "        q1 = np.mean(X_img[:, :14, :14], axis=(1,2)).reshape(-1, 1)\n",
    "        q2 = np.mean(X_img[:, :14, 14:], axis=(1,2)).reshape(-1, 1)\n",
    "        q3 = np.mean(X_img[:, 14:, :14], axis=(1,2)).reshape(-1, 1)\n",
    "        q4 = np.mean(X_img[:, 14:, 14:], axis=(1,2)).reshape(-1, 1)\n",
    "        \n",
    "        # Projections horizontale et verticale (signature du chiffre)\n",
    "        h_proj = (np.sum(X_img, axis=2).max(axis=1) / 28.0).reshape(-1, 1)\n",
    "        v_proj = (np.sum(X_img, axis=1).max(axis=1) / 28.0).reshape(-1, 1)\n",
    "        \n",
    "        # Moments statistiques\n",
    "        skew = np.mean((X_normalized - mean)**3, axis=1, keepdims=True)\n",
    "        \n",
    "        # Centre de masse (position)\n",
    "        y_indices, x_indices = np.meshgrid(np.arange(28), np.arange(28), indexing='ij')\n",
    "        y_indices = y_indices.flatten()\n",
    "        x_indices = x_indices.flatten()\n",
    "        \n",
    "        total_mass = np.sum(X_normalized, axis=1, keepdims=True) + 1e-10\n",
    "        cy = np.sum(X_normalized * y_indices, axis=1, keepdims=True) / total_mass / 28.0\n",
    "        cx = np.sum(X_normalized * x_indices, axis=1, keepdims=True) / total_mass / 28.0\n",
    "        \n",
    "        features = np.hstack([\n",
    "            mean, std, q1, q2, q3, q4, h_proj, v_proj, skew, cy, cx\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Stratégie optimale : ExtraTrees avec sub-sampling stratifié\"\"\"\n",
    "        \n",
    "        if X_train.ndim == 3:\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        \n",
    "        y_train = y_train.ravel()\n",
    "        n_samples = len(X_train)\n",
    "        \n",
    "        # SUB-SAMPLING STRATIFIÉ (clé de la vitesse)\n",
    "        # Garder la distribution des classes\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        sample_ratio = 0.35  # Optimal pour vitesse/précision\n",
    "        indices = []\n",
    "        \n",
    "        for class_label in range(self.output_dim):\n",
    "            class_mask = (y_train == class_label)\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            n_class_samples = int(len(class_indices) * sample_ratio)\n",
    "            sampled = np.random.choice(class_indices, n_class_samples, replace=False)\n",
    "            indices.extend(sampled)\n",
    "        \n",
    "        indices = np.array(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X_sample = X_train[indices]\n",
    "        y_sample = y_train[indices]\n",
    "        \n",
    "        # Features avancées\n",
    "        features = self._extract_advanced_features(X_sample)\n",
    "        \n",
    "        # Normalisation pixels\n",
    "        X_normalized = (X_sample / 255.0).astype(np.float32)\n",
    "        \n",
    "        # Combine pixels + features\n",
    "        X_combined = np.hstack([X_normalized, features])\n",
    "        \n",
    "        # ExtraTrees : Plus rapide que LGBM, précision similaire\n",
    "        self.model = ExtraTreesClassifier(\n",
    "            n_estimators=400,           # Bon équilibre\n",
    "            max_depth=25,               # Profondeur pour capturer complexité\n",
    "            min_samples_split=8,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',        # Randomisation pour robustesse\n",
    "            bootstrap=False,            # ExtraTrees n'utilise pas bootstrap\n",
    "            n_jobs=2,                   # 2 CPUs\n",
    "            random_state=self.seed,\n",
    "            warm_start=False\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_combined, y_sample)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prédiction optimisée\"\"\"\n",
    "        if self.model is None:\n",
    "            return np.zeros(X_test.shape[0], dtype=np.int32)\n",
    "        \n",
    "        if X_test.ndim == 3:\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Features\n",
    "        features = self._extract_advanced_features(X_test)\n",
    "        \n",
    "        # Normalisation\n",
    "        X_normalized = (X_test / 255.0).astype(np.float32)\n",
    "        \n",
    "        # Combine\n",
    "        X_combined = np.hstack([X_normalized, features])\n",
    "        \n",
    "        # Prédiction\n",
    "        predictions = self.model.predict(X_combined)\n",
    "        \n",
    "        return predictions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, output_dim=10, seed=None):\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        self.model = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def _extract_advanced_features(self, X):\n",
    "        \"\"\"Features avancées optimisées pour vitesse\"\"\"\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        X_normalized = X / 255.0\n",
    "        \n",
    "        # Features statistiques de base (ultra-rapides)\n",
    "        mean = np.mean(X_normalized, axis=1, keepdims=True)\n",
    "        std = np.std(X_normalized, axis=1, keepdims=True)\n",
    "        \n",
    "        # Features géométriques (discriminantes)\n",
    "        # Densité par quadrants (28x28 -> 4 zones de 14x14)\n",
    "        X_img = X_normalized.reshape(-1, 28, 28)\n",
    "        q1 = np.mean(X_img[:, :14, :14], axis=(1,2)).reshape(-1, 1)\n",
    "        q2 = np.mean(X_img[:, :14, 14:], axis=(1,2)).reshape(-1, 1)\n",
    "        q3 = np.mean(X_img[:, 14:, :14], axis=(1,2)).reshape(-1, 1)\n",
    "        q4 = np.mean(X_img[:, 14:, 14:], axis=(1,2)).reshape(-1, 1)\n",
    "        \n",
    "        # Projections horizontale et verticale (signature du chiffre)\n",
    "        h_proj = (np.sum(X_img, axis=2).max(axis=1) / 28.0).reshape(-1, 1)\n",
    "        v_proj = (np.sum(X_img, axis=1).max(axis=1) / 28.0).reshape(-1, 1)\n",
    "        \n",
    "        # Moments statistiques\n",
    "        skew = np.mean((X_normalized - mean)**3, axis=1, keepdims=True)\n",
    "        \n",
    "        # Centre de masse (position)\n",
    "        y_indices, x_indices = np.meshgrid(np.arange(28), np.arange(28), indexing='ij')\n",
    "        y_indices = y_indices.flatten()\n",
    "        x_indices = x_indices.flatten()\n",
    "        \n",
    "        total_mass = np.sum(X_normalized, axis=1, keepdims=True) + 1e-10\n",
    "        cy = np.sum(X_normalized * y_indices, axis=1, keepdims=True) / total_mass / 28.0\n",
    "        cx = np.sum(X_normalized * x_indices, axis=1, keepdims=True) / total_mass / 28.0\n",
    "        \n",
    "        features = np.hstack([\n",
    "            mean, std, q1, q2, q3, q4, h_proj, v_proj, skew, cy, cx\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Stratégie optimale : ExtraTrees avec sub-sampling stratifié\"\"\"\n",
    "        \n",
    "        if X_train.ndim == 3:\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        \n",
    "        y_train = y_train.ravel()\n",
    "        n_samples = len(X_train)\n",
    "        \n",
    "        # SUB-SAMPLING STRATIFIÉ (clé de la vitesse)\n",
    "        # Garder la distribution des classes\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        sample_ratio = 0.50  # Optimal pour vitesse/précision\n",
    "        indices = []\n",
    "        \n",
    "        for class_label in range(self.output_dim):\n",
    "            class_mask = (y_train == class_label)\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            n_class_samples = int(len(class_indices) * sample_ratio)\n",
    "            sampled = np.random.choice(class_indices, n_class_samples, replace=False)\n",
    "            indices.extend(sampled)\n",
    "        \n",
    "        indices = np.array(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X_sample = X_train[indices]\n",
    "        y_sample = y_train[indices]\n",
    "        \n",
    "        # Features avancées\n",
    "        features = self._extract_advanced_features(X_sample)\n",
    "        \n",
    "        # Normalisation pixels\n",
    "        X_normalized = (X_sample / 255.0).astype(np.float32)\n",
    "        \n",
    "        # Combine pixels + features\n",
    "        X_combined = np.hstack([X_normalized, features])\n",
    "        \n",
    "        # ExtraTrees : Plus rapide que LGBM, précision similaire\n",
    "        self.model = ExtraTreesClassifier(\n",
    "            n_estimators=600,           # Augmenté pour compenser sub-sampling\n",
    "            max_depth=30,               # Plus profond\n",
    "            min_samples_split=5,        # Moins restrictif\n",
    "            min_samples_leaf=1,         # Maximum de précision\n",
    "            max_features='sqrt',\n",
    "            bootstrap=False,\n",
    "            n_jobs=2,\n",
    "            random_state=self.seed,\n",
    "            warm_start=False\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_combined, y_sample)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prédiction optimisée\"\"\"\n",
    "        if self.model is None:\n",
    "            return np.zeros(X_test.shape[0], dtype=np.int32)\n",
    "        \n",
    "        if X_test.ndim == 3:\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # Features\n",
    "        features = self._extract_advanced_features(X_test)\n",
    "        \n",
    "        # Normalisation\n",
    "        X_normalized = (X_test / 255.0).astype(np.float32)\n",
    "        \n",
    "        # Combine\n",
    "        X_combined = np.hstack([X_normalized, features])\n",
    "        \n",
    "        # Prédiction\n",
    "        predictions = self.model.predict(X_combined)\n",
    "        \n",
    "        return predictions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, output_dim=10, seed=None):\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        self.fast_model = None\n",
    "        self.precise_model = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.fast_model = None\n",
    "        self.precise_model = None\n",
    "    \n",
    "    def _extract_features(self, X):\n",
    "        \"\"\"Features ultra-rapides et discriminantes\"\"\"\n",
    "        if X.ndim == 3:\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        X_norm = X / 255.0\n",
    "        \n",
    "        # 15 features optimales\n",
    "        mean = np.mean(X_norm, axis=1)\n",
    "        std = np.std(X_norm, axis=1)\n",
    "        max_val = np.max(X_norm, axis=1)\n",
    "        min_val = np.min(X_norm, axis=1)\n",
    "        \n",
    "        # Quadrants\n",
    "        X_img = X_norm.reshape(-1, 28, 28)\n",
    "        q1 = np.mean(X_img[:, :14, :14], axis=(1,2))\n",
    "        q2 = np.mean(X_img[:, :14, 14:], axis=(1,2))\n",
    "        q3 = np.mean(X_img[:, 14:, :14], axis=(1,2))\n",
    "        q4 = np.mean(X_img[:, 14:, 14:], axis=(1,2))\n",
    "        \n",
    "        # Projections\n",
    "        h_max = np.max(np.sum(X_img, axis=2), axis=1) / 28.0\n",
    "        v_max = np.max(np.sum(X_img, axis=1), axis=1) / 28.0\n",
    "        \n",
    "        # Densité\n",
    "        dens_high = np.mean(X_norm > 0.5, axis=1)\n",
    "        dens_mid = np.mean((X_norm > 0.2) & (X_norm <= 0.5), axis=1)\n",
    "        \n",
    "        # Moments\n",
    "        skew = np.mean((X_norm - mean[:, None])**3, axis=1)\n",
    "        kurt = np.mean((X_norm - mean[:, None])**4, axis=1)\n",
    "        \n",
    "        # Centre de masse\n",
    "        y_idx, x_idx = np.meshgrid(np.arange(28), np.arange(28), indexing='ij')\n",
    "        y_flat, x_flat = y_idx.flatten(), x_idx.flatten()\n",
    "        total = np.sum(X_norm, axis=1) + 1e-10\n",
    "        cy = np.sum(X_norm * y_flat, axis=1) / total / 28.0\n",
    "        \n",
    "        return np.column_stack([\n",
    "            mean, std, max_val, min_val, q1, q2, q3, q4,\n",
    "            h_max, v_max, dens_high, dens_mid, skew, kurt, cy\n",
    "        ])\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Cascade à 2 étages : Fast filter + Precise classifier\"\"\"\n",
    "        \n",
    "        if X_train.ndim == 3:\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        \n",
    "        y_train = y_train.ravel()\n",
    "        n_samples = len(X_train)\n",
    "        \n",
    "        # === STAGE 1: Fast Filtering (ExtraTrees sur échantillon) ===\n",
    "        # But : Identifier rapidement les 70% d'échantillons \"faciles\"\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        fast_ratio = 0.25  # Entraîne sur 25% seulement\n",
    "        fast_idx = np.random.choice(n_samples, int(n_samples * fast_ratio), replace=False)\n",
    "        \n",
    "        X_fast = X_train[fast_idx]\n",
    "        y_fast = y_train[fast_idx]\n",
    "        \n",
    "        # Features seules pour le fast model (ultra-rapide)\n",
    "        features_fast = self._extract_features(X_fast)\n",
    "        \n",
    "        self.fast_model = ExtraTreesClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_leaf=5,\n",
    "            max_features='log2',\n",
    "            n_jobs=2,\n",
    "            random_state=self.seed\n",
    "        )\n",
    "        \n",
    "        self.fast_model.fit(features_fast, y_fast)\n",
    "        \n",
    "        # === STAGE 2: Precise Classification (LogReg sur TOUS les pixels) ===\n",
    "        # But : Maximiser la précision avec un modèle simple mais puissant\n",
    "        \n",
    "        # Sub-sample stratifié pour le modèle précis\n",
    "        precise_ratio = 0.50\n",
    "        indices = []\n",
    "        \n",
    "        for class_label in range(self.output_dim):\n",
    "            class_mask = (y_train == class_label)\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            n_class = int(len(class_indices) * precise_ratio)\n",
    "            sampled = np.random.choice(class_indices, n_class, replace=False)\n",
    "            indices.extend(sampled)\n",
    "        \n",
    "        indices = np.array(indices)\n",
    "        \n",
    "        X_precise = X_train[indices]\n",
    "        y_precise = y_train[indices]\n",
    "        \n",
    "        # Features + pixels normalisés\n",
    "        features_precise = self._extract_features(X_precise)\n",
    "        X_norm_precise = (X_precise / 255.0).astype(np.float32)\n",
    "        X_combined = np.hstack([X_norm_precise, features_precise])\n",
    "        \n",
    "        # Logistic Regression avec solver ultra-optimisé\n",
    "        self.precise_model = LogisticRegression(\n",
    "            solver='saga',           # Le plus rapide pour gros datasets\n",
    "            max_iter=100,           # Limité pour vitesse\n",
    "            C=0.1,                  # Régularisation forte\n",
    "            multi_class='multinomial',\n",
    "            n_jobs=2,\n",
    "            random_state=self.seed,\n",
    "            warm_start=False\n",
    "        )\n",
    "        \n",
    "        self.precise_model.fit(X_combined, y_precise)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prédiction avec le modèle précis uniquement\"\"\"\n",
    "        \n",
    "        if self.precise_model is None:\n",
    "            return np.zeros(X_test.shape[0], dtype=np.int32)\n",
    "        \n",
    "        if X_test.ndim == 3:\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # On utilise uniquement le precise_model (le fast était juste pour accélérer l'entraînement)\n",
    "        features_test = self._extract_features(X_test)\n",
    "        X_norm_test = (X_test / 255.0).astype(np.float32)\n",
    "        X_combined = np.hstack([X_norm_test, features_test])\n",
    "        \n",
    "        predictions = self.precise_model.predict(X_combined)\n",
    "        \n",
    "        return predictions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "  Downloading auto-sklearn-0.15.0.tar.gz (6.5 MB)\n",
      "     ---------------------------------------- 0.0/6.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/6.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/6.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.5/6.5 MB 810.0 kB/s eta 0:00:08\n",
      "     --- ------------------------------------ 0.5/6.5 MB 810.0 kB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 0.8/6.5 MB 845.5 kB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.0/6.5 MB 852.5 kB/s eta 0:00:07\n",
      "     -------- ------------------------------- 1.3/6.5 MB 955.4 kB/s eta 0:00:06\n",
      "     --------- ------------------------------ 1.6/6.5 MB 1.0 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 1.8/6.5 MB 1.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 2.4/6.5 MB 1.2 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 3.1/6.5 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 3.1/6.5 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 3.9/6.5 MB 1.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 4.5/6.5 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 5.2/6.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 5.5/6.5 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.5/6.5 MB 2.0 MB/s  0:00:03\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [23 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                               \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return hook(config_settings)\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Temp\\pip-build-env-7b2fgvz3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Temp\\pip-build-env-7b2fgvz3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "      \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Temp\\pip-build-env-7b2fgvz3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\ayoub\\AppData\\Local\\Temp\\pip-build-env-7b2fgvz3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m10\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[1;35mValueError\u001b[0m: \u001b[35mDetected unsupported operating system: win32. Please check the compability information of auto-sklearn: https://automl.github.io/auto-sklearn/master/installation.html#windows-osx-compatibility\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\ayoub\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch MLP Agent for Permuted MNIST\n",
    "Multi-layer perceptron with batch normalization\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        hidden_sizes = [400, 400]\n",
    "\n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"PyTorch MLP agent for MNIST classification\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initialize the MLP agent\n",
    "\n",
    "        Args:\n",
    "            output_dim: Number of output classes (10 for MNIST digits)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.model = Model()\n",
    "        self.batch_size = 16\n",
    "        self.validation_fraction = 0.2\n",
    "        self.verbose = True\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent for a new task/simulation\"\"\"\n",
    "        # Reinitialize the model\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train the agent on the provided data\n",
    "\n",
    "        Args:\n",
    "            X_train: Training images (N, 28, 28) or (N, 784)\n",
    "            y_train: Training labels (N, 1) or (N,)\n",
    "        \"\"\"\n",
    "        # Ensure y_train is 1D\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # validation set:\n",
    "        N_val = int(X_train.shape[0] * self.validation_fraction)\n",
    "        X_train_sub, X_val = X_train[N_val:], X_train[:N_val]\n",
    "        y_train_sub, y_val = y_train[N_val:], y_train[:N_val]\n",
    "\n",
    "        X_train_sub = torch.from_numpy(X_train_sub).float() / 255.0\n",
    "        X_val = torch.from_numpy(X_val).float() / 255.0\n",
    "        y_train_sub = torch.from_numpy(y_train_sub).long()\n",
    "        y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "        N = len(X_train_sub)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        for i_epoch in range(10):\n",
    "            perm = np.random.permutation(N)\n",
    "            X = X_train_sub[perm]\n",
    "            Y = y_train_sub[perm]\n",
    "\n",
    "            for i in range(0, N, self.batch_size):\n",
    "                x = X[i:i + self.batch_size]\n",
    "                y = Y[i:i + self.batch_size]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if self.verbose and self.validation_fraction > 0:\n",
    "                y_predict = self.predict(X_val.numpy() * 255.0)\n",
    "                is_correct = y_predict == y_val.numpy()\n",
    "                acc = np.mean(is_correct)\n",
    "                print(f\"epoch {i_epoch}: {acc:0.04f}%\")\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on test data\n",
    "\n",
    "        Args:\n",
    "            X_test: Test images (N, 28, 28) or (N, 784)\n",
    "\n",
    "        Returns:\n",
    "            Class labels (N,)\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        with torch.no_grad():\n",
    "            logits = self.model.forward(X_test)\n",
    "        return logits.argmax(-1).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Architecture plus LÉGÈRE pour la vitesse\n",
    "        hidden_sizes = [256, 256] # (Avant: [512, 512])\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) ÉQUILIBRÉ (Vitesse/Précision).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- RÉGLAGE VITESSE/PRÉCISION ---\n",
    "        self.batch_size = 128      # Rapide sur CPU\n",
    "        self.epochs = 15           # (Avant: 25) Moins d'époques\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 15 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture [256, 256] qui a donné 98.3%\n",
    "        hidden_sizes = [256, 256]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v3 (Réglage final < 60s).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- RÉGLAGE FINAL < 60s ---\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 10           # (Avant: 15) On réduit pour gagner du temps\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [256, 256]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v4 (Test de gros batchs).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- RÉGLAGE VITESSE (GROS BATCHS) ---\n",
    "        self.batch_size = 256      # (Avant: 128) Test de vitesse\n",
    "        self.epochs = 10           # (Inchangé)\n",
    "        # ------------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # --- ARCHITECTURE PLUS LÉGÈRE ---\n",
    "        hidden_sizes = [256, 128] # (Avant: [256, 256])\n",
    "        # --------------------------------\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # --- AJOUT DU DROPOUT ---\n",
    "            layers.append(nn.Dropout(0.5)) # Force la généralisation\n",
    "            # ------------------------\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v6 (Léger + Dropout + Gros Batchs).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON ---\n",
    "        self.batch_size = 256      # (Gros batch pour la vitesse CPU)\n",
    "        self.epochs = 15           # (Plus d'époques, car le modèle est léger)\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train() # Mode entraînement (IMPORTANT pour Dropout)\n",
    "        for i_epoch in range(self.epochs): # 15 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval() # Mode prédiction (DÉSACTIVE Dropout)\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde le [256, 256] qui est rapide\n",
    "        hidden_sizes = [256, 256] \n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # --- ON GARDE LE DROPOUT ---\n",
    "            layers.append(nn.Dropout(0.5)) # Force la généralisation\n",
    "            # ---------------------------\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v7 (Le Bon Équilibre).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON ---\n",
    "        self.batch_size = 256      # (Gros batch pour la vitesse CPU)\n",
    "        self.epochs = 10           # (Rapide, comme le v4)\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train() # Mode entraînement (IMPORTANT pour Dropout)\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval() # Mode prédiction (DÉSACTIVE Dropout)\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # --- ARCHITECTURE ASYMÉTRIQUE ---\n",
    "        hidden_sizes = [512, 256] # (Avant: [256, 256])\n",
    "        # --------------------------------\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # (Pas de Dropout - notre test a prouvé que c'était mauvais)\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v8 (Architecture Asymétrique).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "\n",
    "        self.model = Model()\n",
    "        \n",
    "        # --- RÉGLAGES DE NOTRE MEILLEUR AGENT (v4) ---\n",
    "        self.batch_size = 256      # (Gros batch pour la vitesse CPU)\n",
    "        self.epochs = 10           # (Rapide)\n",
    "        # ---------------------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne l'agent sur 100% DES DONNÉES.\n",
    "        \"\"\"\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture [256, 256]\n",
    "        hidden_sizes = [256, 256]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # --- L'INNOVATION (PRÉCISION) ---\n",
    "            layers.append(nn.Dropout(0.5)) # Le Dropout de Milton\n",
    "            # ---------------------------------\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v10 (Le \"Vrai Milton\")\n",
    "    - Force le Reset()\n",
    "    - Utilise le Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None # Le modèle sera créé dans train()\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 10           # (Vitesse CPU)\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # --- L'INNOVATION (LE RESET FORCÉ) ---\n",
    "        # Le script de test ne le fait pas, donc on le fait.\n",
    "        self.reset()\n",
    "        # -------------------------------------\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train() # Mode entraînement (IMPORTANT pour Dropout)\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval() # Mode prédiction (DÉSACTIVE Dropout)\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # --- ARCHITECTURE ASYMÉTRIQUE (Plus précise) ---\n",
    "        hidden_sizes = [512, 256]\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # --- ON GARDE LE DROPOUT (Précision) ---\n",
    "            layers.append(nn.Dropout(0.5)) \n",
    "            # ---------------------------------\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v11 (Asymétrique + Dropout + 8 époques)\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v11) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 8            # (VITESSE - avant: 10)\n",
    "        # -----------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train() # Mode entraînement (IMPORTANT pour Dropout)\n",
    "        for i_epoch in range(self.epochs): # 8 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval() # Mode prédiction (DÉSACTIVE Dropout)\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # --- ARCHITECTURE 3 COUCHES ---\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        # ------------------------------\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.5)) # On garde le Dropout\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v12 (3 couches + 8 époques).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v12) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 8            # (TRÈS RAPIDE - avant: 10)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 8 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # --- ARCHITECTURE 3 COUCHES (SANS DROPOUT) ---\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            # (PAS DE DROPOUT)\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v13 (3 couches, sans Dropout).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v13) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 10           # (VITESSE - avant: 8)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 10 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture v13 (précise)\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v14 (LR Agressif).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v14) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 8            # (VITESSE)\n",
    "        self.learning_rate = 3e-3  # (AGRESSIF - avant: 1e-3)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate) # Utilise le LR agressif\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 8 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture v13 (la plus précise)\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v15 (AdamW + LR Agressif).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v15) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 9            # (VITESSE - on vise 40s)\n",
    "        self.learning_rate = 3e-3  # (AGRESSIF)\n",
    "        self.weight_decay = 0.01   # (RÉGULARISATION)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # --- OPTIMISEUR INNOVANT ---\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        # ---------------------------\n",
    "        \n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 9 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture v13 (la plus précise)\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2\n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v15 (AdamW + LR Agressif).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v15) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 9            # (VITESSE - on vise 40s)\n",
    "        self.learning_rate = 3e-3  # (AGRESSIF)\n",
    "        self.weight_decay = 0.01   # (RÉGULARISATION)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur 100% des données.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        X_train_tensor = torch.from_numpy(X_train).float() / 255.0\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # --- OPTIMISEUR INNOVANT ---\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        # ---------------------------\n",
    "        \n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 9 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test.\n",
    "        \"\"\"\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float() / 255.0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats # Requis pour les features\n",
    "from sklearn.preprocessing import StandardScaler # Requis pour normaliser les features\n",
    "\n",
    "# ===================================================================\n",
    "# LA FONCTION \"MAGIQUE\" (Ton idée)\n",
    "# ===================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    # Retourne (N_samples, 5)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ===================================================================\n",
    "# LE MODÈLE (789 inputs)\n",
    "# ===================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # On garde l'architecture v4 (98.2% @ 40s)\n",
    "        hidden_sizes = [256, 256]\n",
    "        \n",
    "        layers = []\n",
    "        # --- LA GRANDE INNOVATION ---\n",
    "        d_in = 28 ** 2 + 5 # (784 pixels + 5 features)\n",
    "        # ----------------------------\n",
    "        \n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x est déjà plat (789 features)\n",
    "        return self.model(x)\n",
    "\n",
    "# ===================================================================\n",
    "# L'AGENT \"MILTON\" (v16 - MLP + 5 Features)\n",
    "# ===================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v16 (Pixels + 5 Features).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        self.scaler = None # Pour normaliser les 5 features\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v16) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 8            # (VITESSE - car les features aident)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler() # Normalise les 5 features\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur les 789 features.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 2. Normaliser les deux types de features\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features) # Normalise les 5\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "            \n",
    "        # 4. Préparer PyTorch\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 8 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test (789 features).\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 2. Normaliser\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test) # Utilise le scaler du train\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats # Requis pour les features\n",
    "from sklearn.preprocessing import StandardScaler # Requis pour normaliser les features\n",
    "\n",
    "# ===================================================================\n",
    "# LA FONCTION \"MAGIQUE\" (Inchangée)\n",
    "# ===================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    # Retourne (N_samples, 5)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ===================================================================\n",
    "# LE MODÈLE (789 inputs, Inchangé)\n",
    "# ===================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # On garde l'architecture v4 (98.2% @ 40s)\n",
    "        hidden_sizes = [256, 256]\n",
    "        \n",
    "        layers = []\n",
    "        # --- LA GRANDE INNOVATION ---\n",
    "        d_in = 28 ** 2 + 5 # (784 pixels + 5 features)\n",
    "        # ----------------------------\n",
    "        \n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x est déjà plat (789 features)\n",
    "        return self.model(x)\n",
    "\n",
    "# ===================================================================\n",
    "# L'AGENT \"MILTON\" (v17 - 14 Époques)\n",
    "# ===================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v17 (Pixels + 5 Features + 14 Époques).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        self.scaler = None # Pour normaliser les 5 features\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v17) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 14           # (ON DÉPENSE LE BUDGET TEMPS - avant: 8)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler() # Normalise les 5 features\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur les 789 features.\n",
    "        \"\"\"\n",
    "        # Force le reset\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 2. Normaliser les deux types de features\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features) # Normalise les 5\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "            \n",
    "        # 4. Préparer PyTorch\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 14 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test (789 features).\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 2. Normaliser\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test) # Utilise le scaler du train\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats # Requis pour les features\n",
    "from sklearn.preprocessing import StandardScaler # Requis pour normaliser les features\n",
    "\n",
    "# ===================================================================\n",
    "# LA FONCTION \"MAGIQUE\" (Inchangée)\n",
    "# ===================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales.\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ===================================================================\n",
    "# LE MODÈLE (789 inputs, 3 couches)\n",
    "# ===================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # --- L'ARCHITECTURE LA PLUS PRÉCISE (v13) ---\n",
    "        hidden_sizes = [512, 256, 128]\n",
    "        \n",
    "        layers = []\n",
    "        d_in = 28 ** 2 + 5 # (784 pixels + 5 features)\n",
    "        \n",
    "        for i, n in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n))\n",
    "            layers.append(nn.ReLU())\n",
    "            d_in = n\n",
    "        layers += [nn.Linear(d_in, 10)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ===================================================================\n",
    "# L'AGENT \"MILTON\" (v18 - MLP 3 couches + 5 Features)\n",
    "# ===================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (PyTorch) v18 (3 couches + 5 Features + 8 Époques).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        torch.set_num_threads(2) # 2 CPUs\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "        # --- L'ÉQUILIBRE DE MILTON (v18) ---\n",
    "        self.batch_size = 256      # (Vitesse CPU)\n",
    "        self.epochs = 8            # (VITESSE - on vise 38s)\n",
    "        # -----------------------------------\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Crée un modèle frais.\"\"\"\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Entraîne un NOUVEAU modèle sur les 789 features.\n",
    "        \"\"\"\n",
    "        self.reset() # Force le reset\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "        \n",
    "        # 2. Normaliser les deux types de features\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "            \n",
    "        # 4. Préparer PyTorch\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, # 256\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        for i_epoch in range(self.epochs): # 8 époques\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.model(x)\n",
    "                loss = ce(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fait des prédictions sur les données de test (789 features).\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné.\")\n",
    "            \n",
    "        # 1. Créer les features\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "        \n",
    "        # 2. Normaliser\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        \n",
    "        # 3. Combiner\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x in test_loader:\n",
    "                logits = self.model.forward(x)\n",
    "                preds = logits.argmax(-1).detach().cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                \n",
    "        return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v19 : 7 Features globales\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 7 'super-features' globales (v19).\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    skew = stats.skew(X_flat, axis=1)\n",
    "    kurt = stats.kurtosis(X_flat, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max, skew, kurt)).T\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v19 — 3 couches avec Dropout + BN momentum\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 7  # 784 + 7 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v19 — Max perf 35s CPU (2 threads)\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v19) — 3 couches + 7 features, optimisé pour 2 CPU.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams calibrés pour 35s CPU\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 789 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if time.time() - start > 35:\n",
    "                print(\"⏱️ Temps limite atteint (35s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v19 : 7 Features globales\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 7 'super-features' globales (v19).\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    skew = stats.skew(X_flat, axis=1)\n",
    "    kurt = stats.kurtosis(X_flat, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max, skew, kurt)).T\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v19 — 3 couches avec Dropout + BN momentum\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 7  # 784 + 7 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v19-55s — Max perf 55s CPU (2 threads)\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v19-55s) — 3 couches + 7 features, optimisé pour 2 CPU.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams calibrés pour < 60s CPU\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 789 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # --- MODIFICATION DEMANDÉE ---\n",
    "            if time.time() - start > 41: # Limite passée à 55s\n",
    "                print(\"⏱️ Temps limite atteint (41), arrêt anticipé.\")\n",
    "                break\n",
    "            # ---------------------------\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v20 : Histogramme + Quantiles (RAPIDE)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les features globales v20 (Histogramme 8 bins + 3 Quantiles).\"\"\"\n",
    "    \n",
    "    # 1. Histogramme binné (8 features)\n",
    "    # Calcule l'histogramme pour chaque ligne (image)\n",
    "    # Bins: [0-32, 32-64, 64-96, ..., 224-256]\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "    \n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # Total = 11 features\n",
    "    return np.hstack((\n",
    "        hist_bins, \n",
    "        q1[:, None],      # [:, None] pour transformer (N,) en (N, 1)\n",
    "        median[:, None], \n",
    "        q3[:, None]\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v20 — 3 couches, 11 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 11  # 784 pixels + 11 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v20 — 11 Features, Limite 55s\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v20) — 3 couches + 11 features (Histogramme), 55s.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # On garde les hyperparams rapides\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 795 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Limite de 55 secondes\n",
    "            if time.time() - start > 55:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "#98.50 en 50 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v20 : Histogramme + Quantiles (RAPIDE)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les features globales v20 (Histogramme 8 bins + 3 Quantiles).\"\"\"\n",
    "    \n",
    "    # 1. Histogramme binné (8 features)\n",
    "    # Calcule l'histogramme pour chaque ligne (image)\n",
    "    # Bins: [0-32, 32-64, 64-96, ..., 224-256]\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "    \n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # Total = 11 features\n",
    "    return np.hstack((\n",
    "        hist_bins, \n",
    "        q1[:, None],      # [:, None] pour transformer (N,) en (N, 1)\n",
    "        median[:, None], \n",
    "        q3[:, None]\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v20 — 3 couches, 11 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 11  # 784 pixels + 11 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v20 — 11 Features, Limite 55s\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v20) — 3 couches + 11 features (Histogramme), 55s.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # On garde les hyperparams rapides\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 795 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Limite de 55 secondes\n",
    "            if time.time() - start > 35:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v19 : 7 Features (INCHANGÉ - c'est notre or)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 7 'super-features' globales (v19).\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    skew = stats.skew(X_flat, axis=1)\n",
    "    kurt = stats.kurtosis(X_flat, axis=1)\n",
    "    return np.vstack((mean, std, median, count_zero, count_max, skew, kurt)).T\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v21 — 2 couches (Plus Léger)\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # --- MODIFICATION (Plus Léger) ---\n",
    "        hidden_sizes = [256, 256] # (Avant: [512, 384, 192])\n",
    "        # -------------------------------\n",
    "        \n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 7  # 784 + 7 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v21 — Modèle Léger, Limite 55s\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v21) — 2 couches + 7 features, optimisé pour < 60s.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # On garde les hyperparams (7 époques max, 55s)\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model() # Utilise le nouveau modèle léger\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP (plus léger) sur 791 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # On garde le \"coupe-circuit\" à 55s\n",
    "            if time.time() - start > 55:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v22 : Le \"Paquet Complet\" (18 Features)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 18 'super-features' globales (v22).\"\"\"\n",
    "    \n",
    "    # --- 1. Stats de Base (7 features) ---\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    skew = stats.skew(X_flat, axis=1)\n",
    "    kurt = stats.kurtosis(X_flat, axis=1)\n",
    "\n",
    "    # --- 2. Histogramme (8 features) ---\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "    \n",
    "    # --- 3. Quantiles (3 features) ---\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "    iqr = q3 - q1 # Écart interquartile\n",
    "\n",
    "    # Total = 7 + 8 + 3 = 18 features\n",
    "    return np.hstack((\n",
    "        mean[:, None], std[:, None], median[:, None], \n",
    "        count_zero[:, None], count_max[:, None], skew[:, None], kurt[:, None],\n",
    "        hist_bins, \n",
    "        q1[:, None], q3[:, None], iqr[:, None]\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v22 — 2 couches, 18 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # On garde le modèle léger\n",
    "        hidden_sizes = [256, 256] \n",
    "        dropout = 0.05\n",
    "        d_in = 28 ** 2 + 18  # 784 pixels + 18 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "==\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v22) — 2 couches + 18 features (Le Paquet Complet).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 7 # On garde 7 époques\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model() \n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP (plus léger) sur 802 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # On garde le \"coupe-circuit\" à 55s\n",
    "            if time.time() - start > 55:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n",
    "        #98.4 en 55 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v21 : Histogramme + Quantiles + Spatiales\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les features globales v21 (11 histogramme/quantiles + 8 spatiales).\"\"\"\n",
    "    \n",
    "    # 1. Histogramme binné (8 bins)\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "\n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # 3. Features spatiales légères\n",
    "    X_img = X_flat.reshape(-1, 28, 28)\n",
    "    coords = np.indices((28, 28))\n",
    "    sum_X = np.sum(X_img, axis=(1, 2)) + 1e-6  # éviter division par 0\n",
    "\n",
    "    x_mean = np.sum(coords[1] * X_img, axis=(1, 2)) / sum_X\n",
    "    y_mean = np.sum(coords[0] * X_img, axis=(1, 2)) / sum_X\n",
    "\n",
    "    top = np.mean(X_img[:, :14, :], axis=(1, 2))\n",
    "    bottom = np.mean(X_img[:, 14:, :], axis=(1, 2))\n",
    "    left = np.mean(X_img[:, :, :14], axis=(1, 2))\n",
    "    right = np.mean(X_img[:, :, 14:], axis=(1, 2))\n",
    "\n",
    "    flip_h = np.mean((X_img - np.flip(X_img, axis=2))**2, axis=(1, 2))\n",
    "    flip_v = np.mean((X_img - np.flip(X_img, axis=1))**2, axis=(1, 2))\n",
    "\n",
    "    spatial_feats = np.vstack((x_mean, y_mean, top, bottom, left, right, flip_h, flip_v)).T\n",
    "\n",
    "    # 4. Concaténer toutes les features\n",
    "    return np.hstack((\n",
    "        hist_bins,\n",
    "        q1[:, None],\n",
    "        median[:, None],\n",
    "        q3[:, None],\n",
    "        spatial_feats\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v21 — 3 couches, 19 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28**2 + 19  # 784 pixels + 19 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v21 — 19 Features, ~55s\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v21) — 3 couches + 19 features, optimisé CPU 2 threads.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams calibrés\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 7\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if time.time() - start > 55:\n",
    "                print(\"⏱️ Temps limite atteint (~55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n",
    "#98.6% mais en 73 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v22 — Vectorisé et rapide\n",
    "# ================================================================\n",
    "def create_super_features_fast(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Features globales v22 :\n",
    "    - Histogramme vectorisé 8 bins\n",
    "    - Quantiles 25%, 50%, 75%\n",
    "    - Moyenne haut/bas et gauche/droite (4 features)\n",
    "    \"\"\"\n",
    "    # 1. Histogramme vectorisé\n",
    "    bins = np.arange(0, 257, 32)  # 8 bins\n",
    "    X_clip = np.clip(X_flat, 0, 255).astype(np.int32)\n",
    "    hist = np.zeros((X_flat.shape[0], 8), dtype=np.float32)\n",
    "    for i in range(8):\n",
    "        hist[:, i] = np.sum((X_clip >= bins[i]) & (X_clip < bins[i+1]), axis=1)\n",
    "\n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # 3. Features spatiales rapides\n",
    "    X_img = X_flat.reshape(-1, 28, 28)\n",
    "    top = np.mean(X_img[:, :14, :], axis=(1, 2))\n",
    "    bottom = np.mean(X_img[:, 14:, :], axis=(1, 2))\n",
    "    left = np.mean(X_img[:, :, :14], axis=(1, 2))\n",
    "    right = np.mean(X_img[:, :, 14:], axis=(1, 2))\n",
    "\n",
    "    spatial_feats = np.vstack((top, bottom, left, right)).T\n",
    "\n",
    "    # 4. Concaténer toutes les features\n",
    "    return np.hstack((\n",
    "        hist,\n",
    "        q1[:, None],\n",
    "        median[:, None],\n",
    "        q3[:, None],\n",
    "        spatial_feats\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v22 — 3 couches, 15 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28**2 + 15  # 784 pixels + 15 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v22 — ultra-rapide\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v22) — 3 couches + 15 features vectorisées, optimisé CPU 2 threads.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 7\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = RobustScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        start_total = time.time()\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Feature engineering rapide ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features_fast(X_flat)\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        print(f\"⏱️ Feature engineering: {time.time() - start_total:.2f}s\")\n",
    "\n",
    "        # --- DataLoader ---\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # --- Training loop ---\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            if time.time() - start_total > 35:  # Limite totale\n",
    "                print(\"⏱️ Temps limite total atteint (~35s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "        print(f\"⏱️ Training terminé, temps total: {time.time() - start_total:.2f}s\")\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features_fast(X_test_flat)\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler # RobustScaler est bien aussi\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v20 : Histogramme + Quantiles (RAPIDE)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les features globales v20 (Histogramme 8 bins + 3 Quantiles).\"\"\"\n",
    "    \n",
    "    # 1. Histogramme binné (8 features)\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "    \n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # Total = 11 features (BEAUCOUP plus rapide à calculer)\n",
    "    return np.hstack((\n",
    "        hist_bins, \n",
    "        q1[:, None],\n",
    "        median[:, None], \n",
    "        q3[:, None]\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v20 — 3 couches, 11 features\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28 ** 2 + 11  # 784 pixels + 11 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v20 — 11 Features, ~55s\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v20) — 3 couches + 11 features (RAPIDE), 55s.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # On garde tes hyperparams (ils sont bons)\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 7\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler() # RobustScaler est bien aussi\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 795 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features (RAPIDE) ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Limite de 55 secondes\n",
    "            if time.time() - start > 55:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler # RobustScaler est bien aussi\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v22 : Les 5 Features RAPIDES\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales (LES PLUS RAPIDES).\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    # 5 features, calcul instantané\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v22 — 3 couches (Le Précis)\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture profonde\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28 ** 2 + 5  # 784 pixels + 5 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v22 — L'Hypothèse Finale\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v22) — 3 couches + 5 features (RAPIDES).\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams (on vise 38s)\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 8 # (Avant: 7, mais on a plus de temps)\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler() # RobustScaler est bien aussi\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 789 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features (RAPIDE) ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        # --- PAS DE TIMEOUT ---\n",
    "        # On doit voir le vrai temps (qui doit être ~38s)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler # RobustScaler est bien aussi\n",
    "\n",
    "# ================================================================\n",
    "#  SUPER FEATURES v22 : Les 5 Features RAPIDES (Inchangé)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les 5 'super-features' globales (LES PLUS RAPIDES).\"\"\"\n",
    "    mean = np.mean(X_flat, axis=1)\n",
    "    std = np.std(X_flat, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    count_zero = np.count_nonzero(X_flat == 0, axis=1)\n",
    "    count_max = np.count_nonzero(X_flat == 255, axis=1)\n",
    "    # 5 features, calcul instantané\n",
    "    return np.vstack((mean, std, median, count_zero, count_max)).T\n",
    "\n",
    "# ================================================================\n",
    "#  MLP v22 — 3 couches (Le Précis, Inchangé)\n",
    "# ================================================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # On garde l'architecture profonde\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28 ** 2 + 5  # 784 pixels + 5 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================================================================\n",
    "#  AGENT MILTON v23 — 5 Époques (Pour la Vitesse)\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"Agent MLP (v23) — 3 couches + 5 features (RAPIDES) + 5 Époques.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Hyperparams (on vise < 60s)\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 5 # (AVANT: 8) C'est notre seul changement\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.scaler = StandardScaler() # RobustScaler est bien aussi\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le modèle MLP sur 789 features.\"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features (RAPIDE) ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        # --- PAS DE TIMEOUT ---\n",
    "        # On fait 5 époques complètes\n",
    "\n",
    "        for epoch in range(self.epochs): # 5 époques\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import stats \n",
    "\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    # 1. Histogramme binné (8 bins)\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "\n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # Concaténer toutes les features\n",
    "    return np.hstack((\n",
    "        hist_bins,\n",
    "        q1[:, None],\n",
    "        median[:, None],\n",
    "        q3[:, None],\n",
    "    ))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        hidden_sizes = [512, 384, 192]\n",
    "        dropout = 0.04\n",
    "        d_in = 28 ** 2 + 11  # 784 pixels + 11 features\n",
    "\n",
    "        layers = []\n",
    "        for n in hidden_sizes:\n",
    "            layers.append(nn.Linear(d_in, n))\n",
    "            layers.append(nn.BatchNorm1d(n, momentum=0.3))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            d_in = n\n",
    "        layers.append(nn.Linear(d_in, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        torch.set_num_threads(2)\n",
    "        self.model = None\n",
    "        \n",
    "        # --- REMPLACEMENT DE SKLEARN ---\n",
    "        self.feature_mean = None\n",
    "        self.feature_std = None\n",
    "        # -----------------------------\n",
    "\n",
    "        # Hyperparams calibrés pour la performance\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 7\n",
    "        self.lr = 1.2e-3\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = Model()\n",
    "        self.feature_mean = None\n",
    "        self.feature_std = None\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        self.reset()\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        # 1. Normalisation des Pixels (Pur NumPy)\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        \n",
    "        # 2. Normalisation des Features (Pur NumPy - Remplace sklearn.Scaler)\n",
    "        self.feature_mean = np.mean(super_features, axis=0)\n",
    "        self.feature_std = np.std(super_features, axis=0) + 1e-6 # Stabilité numérique\n",
    "        X_features_norm = (super_features - self.feature_mean) / self.feature_std\n",
    "\n",
    "        # 3. Combinaison\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        X_train_tensor = torch.from_numpy(X_hybrid)\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_train_tensor, y_train_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.epochs\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Limite de 55 secondes (pour la sécurité)\n",
    "            if time.time() - start > 35:\n",
    "                print(\"⏱️ Temps limite atteint (55s), arrêt anticipé.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        # 1. Normalisation des Pixels\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        \n",
    "        # 2. Normalisation des Features (Pur NumPy)\n",
    "        X_features_norm = (super_features_test - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # 3. Combinaison\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        X_test_tensor = torch.from_numpy(X_hybrid_test)\n",
    "        test_loader = DataLoader(X_test_tensor, batch_size=self.batch_size * 2)\n",
    "\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in test_loader:\n",
    "                out = self.model(xb)\n",
    "                preds.append(out.argmax(1).cpu().numpy())\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy import stats # Requis pour les \"super-features\"\n",
    "\n",
    "# ================================================================\n",
    "#  1. FONCTIONNALITÉS HYBRIDES (Identique à BestAgentMLP)\n",
    "#  (numpy et scipy sont des dépendances standard de sklearn)\n",
    "# ================================================================\n",
    "def create_super_features(X_flat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crée les features globales v20 (Histogramme 8 bins + 3 Quantiles).\"\"\"\n",
    "    \n",
    "    # 1. Histogramme binné (8 features)\n",
    "    hist_bins = np.apply_along_axis(\n",
    "        lambda x: np.histogram(x, bins=8, range=(0, 256))[0],\n",
    "        1,\n",
    "        X_flat\n",
    "    )\n",
    "    \n",
    "    # 2. Quantiles (3 features)\n",
    "    q1 = np.percentile(X_flat, 25, axis=1)\n",
    "    median = np.median(X_flat, axis=1)\n",
    "    q3 = np.percentile(X_flat, 75, axis=1)\n",
    "\n",
    "    # Total = 11 features\n",
    "    return np.hstack((\n",
    "        hist_bins, \n",
    "        q1[:, None],\n",
    "        median[:, None], \n",
    "        q3[:, None]\n",
    "    ))\n",
    "\n",
    "# ================================================================\n",
    "#  2. CLASSE AGENT (100% Sklearn)\n",
    "# ================================================================\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent MLP 100% Sklearn (MLPClassifier) avec \"super-features\".\n",
    "    Entraîné depuis un \"Cold Start\" (pas de Warm Start JSON).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int = 10, seed: int = None):\n",
    "        self.output_dim = output_dim # output_dim n'est pas utilisé par MLPClassifier\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.scaler = None # Scaler pour les 11 \"super-features\"\n",
    "        \n",
    "        # --- HYPERPARAMÈTRES OPTIMISÉS (MLPClassifier) ---\n",
    "        self.mlp_params = {\n",
    "            'hidden_layer_sizes': (512, 384, 192), # Similaire à votre MLP PyTorch\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam',\n",
    "            'batch_size': 256,\n",
    "            'learning_rate_init': 1e-3,\n",
    "            'random_state': self.seed,\n",
    "            'warm_start': False, # Entraînement depuis 0 à chaque tâche\n",
    "            \n",
    "            # --- C'est ici que vous contrôlez le temps ---\n",
    "            # Viser 99% en ~55s (au lieu de 98.7% en 35s)\n",
    "            # 1. Augmenter le nombre max d'époques\n",
    "            'max_iter': 15, # (15 époques au lieu des 7 de votre BestAgentMLP)\n",
    "            # 2. Désactiver l'Early Stopping pour utiliser tout le temps\n",
    "            'tol': 1e-9, \n",
    "            'n_iter_no_change': 15 # (Désactive l'early stopping)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Initialise un nouveau modèle et un nouveau scaler pour la tâche.\"\"\"\n",
    "        self.model = MLPClassifier(**self.mlp_params)\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Entraîne le MLPClassifier sur les 784 pixels + 11 features.\"\"\"\n",
    "        \n",
    "        # NOTE : La contrainte de 60s s'applique à toute cette fonction 'train'\n",
    "        # MLPClassifier n'a pas de \"coupe-circuit\" temporel, \n",
    "        # 'max_iter' DOIT être réglé pour finir en < 55s.\n",
    "        \n",
    "        self.reset() # Nouveau modèle à chaque tâche\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "        # --- Préparation features ---\n",
    "        X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        super_features = create_super_features(X_flat)\n",
    "\n",
    "        # Normaliser les pixels (784 features)\n",
    "        X_pixels_norm = X_flat / 255.0\n",
    "        \n",
    "        # Normaliser les super-features (11 features)\n",
    "        X_features_norm = self.scaler.fit_transform(super_features)\n",
    "\n",
    "        # Combiner (795 features)\n",
    "        X_hybrid = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "        \n",
    "        # Entraînement (MLPClassifier gère le temps via max_iter)\n",
    "        self.model.fit(X_hybrid, y_train)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prédit les classes sur X_test.\"\"\"\n",
    "        if self.model is None or self.scaler is None:\n",
    "            raise RuntimeError(\"L'agent doit être entraîné avant prédiction.\")\n",
    "\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        # 1. Créer les super-features\n",
    "        super_features_test = create_super_features(X_test_flat)\n",
    "\n",
    "        # 2. Normaliser les pixels\n",
    "        X_pixels_norm = X_test_flat / 255.0\n",
    "        \n",
    "        # 3. Normaliser les super-features (avec le scaler FITTÉ)\n",
    "        X_features_norm = self.scaler.transform(super_features_test)\n",
    "        \n",
    "        # 4. Combiner\n",
    "        X_hybrid_test = np.hstack((X_pixels_norm, X_features_norm)).astype(np.float32)\n",
    "\n",
    "        # 5. Prédire\n",
    "        return self.model.predict(X_hybrid_test)\n",
    "\n",
    "    # (save/load ne sont pas nécessaires pour la soumission en \"Cold Start\")\n",
    "    def save(self, path: str = None):\n",
    "        pass\n",
    "\n",
    "    def load(self, path: str = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
